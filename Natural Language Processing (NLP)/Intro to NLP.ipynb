{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in text-cleaning for Natural Language Processing (NLP) problems\n",
    "\n",
    "When working with text data for supervised Machine-Learning or deep learning problems, text data needs to be handled appropriately with the following key steps:\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "2. Removing stopwords and special characters that are irrelevant and converting all characters to lower case.\n",
    "\n",
    "3. Stemming/Lemmatization\n",
    "\n",
    "4. Word Embeddings\n",
    "\n",
    "Note that the steps above may be customized depending on the given problem statement.\n",
    "\n",
    "In NLP, the following terminologies are frequently used:\n",
    "\n",
    "1. Corpus: Paragraph with more than 1 sentence\n",
    "\n",
    "2. Documents: List of sentences\n",
    "\n",
    "3. Vocabulary: Number of unique words (in dictionary form)\n",
    "\n",
    "4. Words: Individual alphabets that provides meaning when combined together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, ConfusionMatrixDisplay\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of converting sentences into individual words. This is the most fundamental step that allows words to be further processed based on its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'twillis',\n",
       " 'ec',\n",
       " 'ecn',\n",
       " 'purdue',\n",
       " 'edu',\n",
       " 'thomas',\n",
       " 'e',\n",
       " 'willis',\n",
       " 'subject',\n",
       " 'pb',\n",
       " 'questions',\n",
       " 'organization',\n",
       " 'purdue',\n",
       " 'university',\n",
       " 'engineering',\n",
       " 'computer',\n",
       " 'network',\n",
       " 'distribution',\n",
       " 'usa',\n",
       " 'lines',\n",
       " 'well',\n",
       " 'folks',\n",
       " 'my',\n",
       " 'mac',\n",
       " 'plus',\n",
       " 'finally',\n",
       " 'gave',\n",
       " 'up',\n",
       " 'the',\n",
       " 'ghost',\n",
       " 'this',\n",
       " 'weekend',\n",
       " 'after',\n",
       " 'starting',\n",
       " 'life',\n",
       " 'as',\n",
       " 'a',\n",
       " 'k',\n",
       " 'way',\n",
       " 'back',\n",
       " 'in',\n",
       " 'sooo',\n",
       " 'i',\n",
       " 'm',\n",
       " 'in',\n",
       " 'the',\n",
       " 'market',\n",
       " 'for',\n",
       " 'a',\n",
       " 'new',\n",
       " 'machine',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'sooner',\n",
       " 'than',\n",
       " 'i',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'be',\n",
       " 'i',\n",
       " 'm',\n",
       " 'looking',\n",
       " 'into',\n",
       " 'picking',\n",
       " 'up',\n",
       " 'a',\n",
       " 'powerbook',\n",
       " 'or',\n",
       " 'maybe',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'questions',\n",
       " 'that',\n",
       " 'hopefully',\n",
       " 'somebody',\n",
       " 'can',\n",
       " 'answer',\n",
       " 'does',\n",
       " 'anybody',\n",
       " 'know',\n",
       " 'any',\n",
       " 'dirt',\n",
       " 'on',\n",
       " 'when',\n",
       " 'the',\n",
       " 'next',\n",
       " 'round',\n",
       " 'of',\n",
       " 'powerbook',\n",
       " 'introductions',\n",
       " 'are',\n",
       " 'expected',\n",
       " 'i',\n",
       " 'd',\n",
       " 'heard',\n",
       " 'the',\n",
       " 'c',\n",
       " 'was',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'make',\n",
       " 'an',\n",
       " 'appearence',\n",
       " 'this',\n",
       " 'summer',\n",
       " 'but',\n",
       " 'haven',\n",
       " 't',\n",
       " 'heard',\n",
       " 'anymore',\n",
       " 'on',\n",
       " 'it',\n",
       " 'and',\n",
       " 'since',\n",
       " 'i',\n",
       " 'don',\n",
       " 't',\n",
       " 'have',\n",
       " 'access',\n",
       " 'to',\n",
       " 'macleak',\n",
       " 'i',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anybody',\n",
       " 'out',\n",
       " 'there',\n",
       " 'had',\n",
       " 'more',\n",
       " 'info',\n",
       " 'has',\n",
       " 'anybody',\n",
       " 'heard',\n",
       " 'rumors',\n",
       " 'about',\n",
       " 'price',\n",
       " 'drops',\n",
       " 'to',\n",
       " 'the',\n",
       " 'powerbook',\n",
       " 'line',\n",
       " 'like',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'the',\n",
       " 'duo',\n",
       " 's',\n",
       " 'just',\n",
       " 'went',\n",
       " 'through',\n",
       " 'recently',\n",
       " 'what',\n",
       " 's',\n",
       " 'the',\n",
       " 'impression',\n",
       " 'of',\n",
       " 'the',\n",
       " 'display',\n",
       " 'on',\n",
       " 'the',\n",
       " 'i',\n",
       " 'could',\n",
       " 'probably',\n",
       " 'swing',\n",
       " 'a',\n",
       " 'if',\n",
       " 'i',\n",
       " 'got',\n",
       " 'the',\n",
       " 'mb',\n",
       " 'disk',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'the',\n",
       " 'but',\n",
       " 'i',\n",
       " 'don',\n",
       " 't',\n",
       " 'really',\n",
       " 'have',\n",
       " 'a',\n",
       " 'feel',\n",
       " 'for',\n",
       " 'how',\n",
       " 'much',\n",
       " 'better',\n",
       " 'the',\n",
       " 'display',\n",
       " 'is',\n",
       " 'yea',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'great',\n",
       " 'in',\n",
       " 'the',\n",
       " 'store',\n",
       " 'but',\n",
       " 'is',\n",
       " 'that',\n",
       " 'all',\n",
       " 'wow',\n",
       " 'or',\n",
       " 'is',\n",
       " 'it',\n",
       " 'really',\n",
       " 'that',\n",
       " 'good',\n",
       " 'could',\n",
       " 'i',\n",
       " 'solicit',\n",
       " 'some',\n",
       " 'opinions',\n",
       " 'of',\n",
       " 'people',\n",
       " 'who',\n",
       " 'use',\n",
       " 'the',\n",
       " 'and',\n",
       " 'day',\n",
       " 'to',\n",
       " 'day',\n",
       " 'on',\n",
       " 'if',\n",
       " 'its',\n",
       " 'worth',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'disk',\n",
       " 'size',\n",
       " 'and',\n",
       " 'money',\n",
       " 'hit',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'active',\n",
       " 'display',\n",
       " 'i',\n",
       " 'realize',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'real',\n",
       " 'subjective',\n",
       " 'question',\n",
       " 'but',\n",
       " 'i',\n",
       " 've',\n",
       " 'only',\n",
       " 'played',\n",
       " 'around',\n",
       " 'with',\n",
       " 'the',\n",
       " 'machines',\n",
       " 'in',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'store',\n",
       " 'breifly',\n",
       " 'and',\n",
       " 'figured',\n",
       " 'the',\n",
       " 'opinions',\n",
       " 'of',\n",
       " 'somebody',\n",
       " 'who',\n",
       " 'actually',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'daily',\n",
       " 'might',\n",
       " 'prove',\n",
       " 'helpful',\n",
       " 'how',\n",
       " 'well',\n",
       " 'does',\n",
       " 'hellcats',\n",
       " 'perform',\n",
       " 'thanks',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'in',\n",
       " 'advance',\n",
       " 'for',\n",
       " 'any',\n",
       " 'info',\n",
       " 'if',\n",
       " 'you',\n",
       " 'could',\n",
       " 'email',\n",
       " 'i',\n",
       " 'll',\n",
       " 'post',\n",
       " 'a',\n",
       " 'summary',\n",
       " 'news',\n",
       " 'reading',\n",
       " 'time',\n",
       " 'is',\n",
       " 'at',\n",
       " 'a',\n",
       " 'premium',\n",
       " 'with',\n",
       " 'finals',\n",
       " 'just',\n",
       " 'around',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'tom',\n",
       " 'willis',\n",
       " 'twillis',\n",
       " 'ecn',\n",
       " 'purdue',\n",
       " 'edu',\n",
       " 'purdue',\n",
       " 'electrical',\n",
       " 'engineering',\n",
       " 'convictions',\n",
       " 'are',\n",
       " 'more',\n",
       " 'dangerous',\n",
       " 'enemies',\n",
       " 'of',\n",
       " 'truth',\n",
       " 'than',\n",
       " 'lies',\n",
       " 'f',\n",
       " 'w',\n",
       " 'nietzsche']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_tokenization(text):\n",
    "    review = re.sub('[^a-zA-Z]',' ', text).lower().split()\n",
    "    return review\n",
    "corpus = pd.Series(train_data.data).map(lambda x: text_tokenization(str(x)))\n",
    "corpus[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords and irrelevant special characters\n",
    "\n",
    "Stopwords refer to list of less important words that can be removed. While nltk library has its own list of stopwords, stopwords can also be customized depending on the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization\n",
    "\n",
    "<b>Stemming</b>: Process of converting complex words into its base word stem\n",
    "\n",
    "Note that stemming process may remove meaning of original word despite being a fast process.\n",
    "\n",
    "<b>Lemmatization</b>: Process of converting complex words into its base word stem and ensures base word belongs to given language.\n",
    "\n",
    "Note that lemmatization process retains meaning of original word however it is a slower process than stemming.\n",
    "\n",
    "In cases where grammar of sentence is emphasized like chatbots, language translation or text summarization, lemmatization is a more suitable alternative than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [07:18<00:00, 25.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd edu thing subject car nntp post host rac wam umd edu organ univers maryland colleg park line wonder anyon could enlighten car saw day door sport car look late earli call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car made histori whatev info funki look car pleas e mail thank il brought neighborhood lerxst'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "tqdm.pandas()\n",
    "def text_stemming(text):\n",
    "    review = re.sub('[^a-zA-Z]',' ', text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "corpus = pd.Series(train_data.data).progress_map(lambda x: text_stemming(str(x)))\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [06:44<00:00, 27.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd edu thing subject car nntp posting host rac wam umd edu organization university maryland college park line wondering anyone could enlighten car saw day door sport car looked late early called bricklin door really small addition front bumper separate rest body know anyone tellme model name engine spec year production car made history whatever info funky looking car please e mail thanks il brought neighborhood lerxst'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "tqdm.pandas()\n",
    "def text_lemmatize(text):\n",
    "    review = re.sub('[^a-zA-Z]',' ', text).lower().split()\n",
    "    review = [wnl.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "corpus = pd.Series(train_data.data).progress_map(lambda x: text_lemmatize(str(x)))\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embedding is a general process of converting words into vectors of numbers for machine learning/deep learning models to process.\n",
    "\n",
    "Word embeddings can be generally classified into two main domains:\n",
    "\n",
    "1. <b>Count/Frequency-based</b>\n",
    "\n",
    "- One Hot Encoding\n",
    "- Bag of Words\n",
    "- Term Frequency - Inverse Document Frequency (TFIDF)\n",
    "\n",
    "2. <b>Deep Learning</b>\n",
    "\n",
    "- Word2Vec\n",
    "- AverageWord2Vec\n",
    "- Doc2Vec\n",
    "- RNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "In NLP, this method involves assigning binary indicators to every word in the vocabulary.\n",
    "\n",
    "<b>Advantages</b>: Simple to implement and intuitive\n",
    "\n",
    "<b>Disadvantages</b>:\n",
    "\n",
    "1. Sparse matrix\n",
    "\n",
    "2. Out of vocabulary (No fixed size sentences)\n",
    "\n",
    "3. Semantic meaning between words is not captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Bag of words involves converting sentences into features that represent frequency of words in a given sentence. Features are sorted based on frequency from highest to lowest.\n",
    "\n",
    "<img src=\"https://user.oc-static.com/upload/2020/10/23/16034397439042_surfin%20bird%20bow.png\" width=\"600\">\n",
    "\n",
    "<b>Note that Bag of Words in sklearn has the option to make it strictly-binary for words with more than 1 occurence in a given sentence.</b>\n",
    "\n",
    "<b>Advantages</b>: Simple to implement and intuitive\n",
    "\n",
    "<b>Disadvantages</b>:\n",
    "\n",
    "1. Sparse matrix\n",
    "2. Out of vocabulary (No fixed size sentences)\n",
    "3. Meaning of sentence may be distorted due to changes in order of words\n",
    "4. Semantic information (similarity) between words is not captured (using unigram)\n",
    "\n",
    "Semantic meaning between words for bag of words can be captured by using the concept of n-grams.\n",
    "\n",
    "N-grams refer to bundling n sequential words per feature (i.e. A B C D E -> AB, BC, CD, DE (for 4 bi-grams))\n",
    "\n",
    "<b>Important hyperparameters for Bag of Words (CountVectorizer)</b>:\n",
    "1. max_features: Building vocabulary that contains top max_features sorted by frequency in descending order.\n",
    "2. ngram_range: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "3. binary: Indicator of whether bag-of-words should be strictly binary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>ac</th>\n",
       "      <th>access</th>\n",
       "      <th>actually</th>\n",
       "      <th>address</th>\n",
       "      <th>also</th>\n",
       "      <th>always</th>\n",
       "      <th>american</th>\n",
       "      <th>another</th>\n",
       "      <th>answer</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>writes</th>\n",
       "      <th>writes article</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       able  ac  access  actually  address  also  always  american  another  \\\n",
       "0         0   0       0         0        0     0       0         0        0   \n",
       "1         0   0       0         0        0     0       0         0        0   \n",
       "2         0   0       1         1        0     0       0         0        0   \n",
       "3         0   0       0         0        1     0       0         0        0   \n",
       "4         0   0       0         0        0     0       0         0        0   \n",
       "...     ...  ..     ...       ...      ...   ...     ...       ...      ...   \n",
       "11309     1   0       0         0        0     3       0         0        0   \n",
       "11310     0   0       0         0        0     0       0         0        0   \n",
       "11311     0   0       0         0        0     0       0         0        0   \n",
       "11312     0   0       0         0        0     0       0         0        0   \n",
       "11313     0   0       0         0        0     0       0         0        0   \n",
       "\n",
       "       answer  ...  word  work  world  would  writes  writes article  wrong  \\\n",
       "0           0  ...     0     0      0      0       0               0      0   \n",
       "1           0  ...     0     0      0      0       0               0      0   \n",
       "2           1  ...     0     0      0      0       0               0      0   \n",
       "3           0  ...     0     0      1      0       1               1      0   \n",
       "4           0  ...     0     0      2      0       1               0      0   \n",
       "...       ...  ...   ...   ...    ...    ...     ...             ...    ...   \n",
       "11309       0  ...     0     0      1      0       0               0      0   \n",
       "11310       0  ...     0     0      0      0       0               0      0   \n",
       "11311       0  ...     0     1      0      0       0               0      0   \n",
       "11312       0  ...     0     0      0      0       1               0      1   \n",
       "11313       0  ...     0     0      0      0       0               0      0   \n",
       "\n",
       "       year  yes  yet  \n",
       "0         1    0    0  \n",
       "1         0    0    0  \n",
       "2         0    0    0  \n",
       "3         0    0    0  \n",
       "4         0    1    1  \n",
       "...     ...  ...  ...  \n",
       "11309     1    0    0  \n",
       "11310     0    0    0  \n",
       "11311     0    0    1  \n",
       "11312     0    0    0  \n",
       "11313     0    0    0  \n",
       "\n",
       "[11314 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of words model\n",
    "cv = CountVectorizer(max_features=300, ngram_range=(1,3))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "data = pd.DataFrame(X, columns = sorted(cv.vocabulary_, key=cv.vocabulary_.get))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nested_cv(X, y, pipeline, search_space = None):\n",
    "    num_folds = 10\n",
    "    skfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=120)\n",
    "    val_f1, test_f1 = [], []\n",
    "    for fold, (outer_train_idx, outer_test_idx) in enumerate(skfold.split(X, y)):\n",
    "        X_train = X.iloc[outer_train_idx].reset_index(drop=True)\n",
    "        y_train = y.iloc[outer_train_idx].reset_index(drop=True)\n",
    "        X_test = X.iloc[outer_test_idx].reset_index(drop=True)\n",
    "        y_test = y.iloc[outer_test_idx].reset_index(drop=True)\n",
    "        search = BayesSearchCV(estimator=pipeline, search_spaces=search_space, cv=3, n_iter=10,scoring= make_scorer(f1_score, average='macro'),refit=True, n_jobs=3)\n",
    "        search.fit(X_train,y_train)\n",
    "        val_f1.append(search.best_score_)\n",
    "        print(f'Validation F1 score for fold {fold+1}:',search.best_score_)\n",
    "        print(f'Best hyperparameters for fold {fold+1}:', search.best_params_)\n",
    "        y_pred = search.best_estimator_.predict(X_test)\n",
    "        test_f1.append(f1_score(y_test,y_pred, average='macro'))\n",
    "        print(f'Test F1 score for fold {fold+1}:',f1_score(y_test,y_pred, average='macro'))\n",
    "        print()\n",
    "    print('----------------------')\n",
    "    print('Average validation F1 score:', np.mean(val_f1))\n",
    "    print('Average test F1 score:', np.mean(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score for fold 1: 0.18392029731265216\n",
      "Best hyperparameters for fold 1: OrderedDict([('classification__ccp_alpha', 0.002248774476935585), ('classification__class_weight', None), ('text__max_features', 94)])\n",
      "Test F1 score for fold 1: 0.15752156904390618\n",
      "\n",
      "Validation F1 score for fold 2: 0.14315626710130913\n",
      "Best hyperparameters for fold 2: OrderedDict([('classification__ccp_alpha', 0.004836949011754973), ('classification__class_weight', None), ('text__max_features', 96)])\n",
      "Test F1 score for fold 2: 0.11996673687303158\n",
      "\n",
      "Validation F1 score for fold 3: 0.1503670391660286\n",
      "Best hyperparameters for fold 3: OrderedDict([('classification__ccp_alpha', 0.0044015787973292544), ('classification__class_weight', None), ('text__max_features', 94)])\n",
      "Test F1 score for fold 3: 0.13797332912150556\n",
      "\n",
      "Validation F1 score for fold 4: 0.22948909596900116\n",
      "Best hyperparameters for fold 4: OrderedDict([('classification__ccp_alpha', 0.0008963686954806895), ('classification__class_weight', None), ('text__max_features', 92)])\n",
      "Test F1 score for fold 4: 0.21735917524118992\n",
      "\n",
      "Validation F1 score for fold 5: 0.09595962823269048\n",
      "Best hyperparameters for fold 5: OrderedDict([('classification__ccp_alpha', 0.005239038653042156), ('classification__class_weight', None), ('text__max_features', 72)])\n",
      "Test F1 score for fold 5: 0.10382735353845747\n",
      "\n",
      "Validation F1 score for fold 6: 0.24231604072635218\n",
      "Best hyperparameters for fold 6: OrderedDict([('classification__ccp_alpha', 0.00098388484962366), ('classification__class_weight', 'balanced'), ('text__max_features', 95)])\n",
      "Test F1 score for fold 6: 0.22757977765915882\n",
      "\n",
      "Validation F1 score for fold 7: 0.10157238304753728\n",
      "Best hyperparameters for fold 7: OrderedDict([('classification__ccp_alpha', 0.007238959205033943), ('classification__class_weight', 'balanced'), ('text__max_features', 81)])\n",
      "Test F1 score for fold 7: 0.07993723065125832\n",
      "\n",
      "Validation F1 score for fold 8: 0.10992541392375439\n",
      "Best hyperparameters for fold 8: OrderedDict([('classification__ccp_alpha', 0.0048145100961286625), ('classification__class_weight', 'balanced'), ('text__max_features', 86)])\n",
      "Test F1 score for fold 8: 0.1003088263463003\n",
      "\n",
      "Validation F1 score for fold 9: 0.2086672313269824\n",
      "Best hyperparameters for fold 9: OrderedDict([('classification__ccp_alpha', 3.2165257955139035e-05), ('classification__class_weight', None), ('text__max_features', 64)])\n",
      "Test F1 score for fold 9: 0.2442363527964318\n",
      "\n",
      "Validation F1 score for fold 10: 0.12407993911530302\n",
      "Best hyperparameters for fold 10: OrderedDict([('classification__ccp_alpha', 3.675311066714705e-05), ('classification__class_weight', None), ('text__max_features', 16)])\n",
      "Test F1 score for fold 10: 0.14118285041575077\n",
      "\n",
      "----------------------\n",
      "Average validation F1 score: 0.15894533359216106\n",
      "Average test F1 score: 0.1529893201686991\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=120)\n",
    "pipeline = Pipeline(steps=[])\n",
    "pipeline.steps.append(('text',CountVectorizer(ngram_range=(1,2))))\n",
    "pipeline.steps.append(('classification',clf))\n",
    "search_space = dict()\n",
    "search_space['text__max_features'] = Integer(1,100)\n",
    "search_space['classification__ccp_alpha'] = Real(0,0.02)\n",
    "search_space['classification__class_weight'] = Categorical(['balanced',None])\n",
    "nested_cv(corpus, pd.Series(train_data.target), pipeline, search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "Unlike Bag of Words, TFIDF captures semantic meaning of words directly by providing more weightage to less frequent words.\n",
    "\n",
    "![image.png](https://blog.expertrec.com/wp-content/uploads/2019/02/TF-IDF-calucation.png)\n",
    "\n",
    "TFIDF consists of two components with its respective formulas:\n",
    "\n",
    "1. Term Frequency (TF) : Num. of words repeated/Num. of words\n",
    "\n",
    "2. Inverse Document Frequency (IDF): log(Num. of sentences/Num. of sentences that contains specified word)\n",
    "\n",
    "Note that TF component captures less frequent words, while IDF component captures more frequent words.\n",
    "\n",
    "TFIDF is simply multiplying both TF and IDF components together.\n",
    "\n",
    "<b>Advantages</b>: Intuitive and captures word importance\n",
    "\n",
    "<b>Disadvantages</b>:\n",
    "\n",
    "1. Sparse matrix\n",
    "\n",
    "2. Out of vocabulary (No fixed size sentences)\n",
    "\n",
    "<b>Important hyperparameters for TFIDF (TfidfVectorizer)</b>:\n",
    "1. max_features: Building vocabulary that contains top max_features sorted by term-frequency in descending order.\n",
    "2. ngram_range: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "3. binary: Indicator of whether tfidf should be strictly binary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>ac</th>\n",
       "      <th>access</th>\n",
       "      <th>actually</th>\n",
       "      <th>address</th>\n",
       "      <th>also</th>\n",
       "      <th>always</th>\n",
       "      <th>american</th>\n",
       "      <th>another</th>\n",
       "      <th>answer</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>writes</th>\n",
       "      <th>writes article</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107916</td>\n",
       "      <td>0.101784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.158146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214153</td>\n",
       "      <td>0.213929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>0.144964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           able   ac    access  actually   address      also  always  \\\n",
       "0      0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "1      0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "2      0.000000  0.0  0.107916  0.101784  0.000000  0.000000     0.0   \n",
       "3      0.000000  0.0  0.000000  0.000000  0.193904  0.000000     0.0   \n",
       "4      0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "...         ...  ...       ...       ...       ...       ...     ...   \n",
       "11309  0.144964  0.0  0.000000  0.000000  0.000000  0.298494     0.0   \n",
       "11310  0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "11311  0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "11312  0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "11313  0.000000  0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "\n",
       "       american  another    answer  ...  word      work     world  would  \\\n",
       "0           0.0      0.0  0.000000  ...   0.0  0.000000  0.000000    0.0   \n",
       "1           0.0      0.0  0.000000  ...   0.0  0.000000  0.000000    0.0   \n",
       "2           0.0      0.0  0.111287  ...   0.0  0.000000  0.000000    0.0   \n",
       "3           0.0      0.0  0.000000  ...   0.0  0.000000  0.133627    0.0   \n",
       "4           0.0      0.0  0.000000  ...   0.0  0.000000  0.322645    0.0   \n",
       "...         ...      ...       ...  ...   ...       ...       ...    ...   \n",
       "11309       0.0      0.0  0.000000  ...   0.0  0.000000  0.111044    0.0   \n",
       "11310       0.0      0.0  0.000000  ...   0.0  0.000000  0.000000    0.0   \n",
       "11311       0.0      0.0  0.000000  ...   0.0  0.160145  0.000000    0.0   \n",
       "11312       0.0      0.0  0.000000  ...   0.0  0.000000  0.000000    0.0   \n",
       "11313       0.0      0.0  0.000000  ...   0.0  0.000000  0.000000    0.0   \n",
       "\n",
       "         writes  writes article     wrong      year       yes       yet  \n",
       "0      0.000000        0.000000  0.000000  0.113780  0.000000  0.000000  \n",
       "1      0.000000        0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2      0.000000        0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3      0.080919        0.158146  0.000000  0.000000  0.000000  0.000000  \n",
       "4      0.097690        0.000000  0.000000  0.000000  0.214153  0.213929  \n",
       "...         ...             ...       ...       ...       ...       ...  \n",
       "11309  0.000000        0.000000  0.000000  0.108469  0.000000  0.000000  \n",
       "11310  0.000000        0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "11311  0.000000        0.000000  0.000000  0.000000  0.000000  0.211890  \n",
       "11312  0.056444        0.000000  0.122216  0.000000  0.000000  0.000000  \n",
       "11313  0.000000        0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[11314 rows x 300 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF model\n",
    "cv = TfidfVectorizer(max_features=300, ngram_range=(1,3))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "data = pd.DataFrame(X, columns = sorted(cv.vocabulary_, key=cv.vocabulary_.get))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score for fold 1: 0.15067997172854272\n",
      "Best hyperparameters for fold 1: OrderedDict([('classification__ccp_alpha', 0.00020614205652772058), ('classification__class_weight', 'balanced'), ('text__max_features', 44)])\n",
      "Test F1 score for fold 1: 0.1518826204103875\n",
      "\n",
      "Validation F1 score for fold 2: 0.13800662344050796\n",
      "Best hyperparameters for fold 2: OrderedDict([('classification__ccp_alpha', 0.00546314958659191), ('classification__class_weight', None), ('text__max_features', 95)])\n",
      "Test F1 score for fold 2: 0.12635475974516777\n",
      "\n",
      "Validation F1 score for fold 3: 0.16958932386649783\n",
      "Best hyperparameters for fold 3: OrderedDict([('classification__ccp_alpha', 0.002015986738326026), ('classification__class_weight', 'balanced'), ('text__max_features', 89)])\n",
      "Test F1 score for fold 3: 0.16048181336410303\n",
      "\n",
      "Validation F1 score for fold 4: 0.051044871304274646\n",
      "Best hyperparameters for fold 4: OrderedDict([('classification__ccp_alpha', 0.016845250201640002), ('classification__class_weight', 'balanced'), ('text__max_features', 79)])\n",
      "Test F1 score for fold 4: 0.06165161892170792\n",
      "\n",
      "Validation F1 score for fold 5: 0.1602506803949611\n",
      "Best hyperparameters for fold 5: OrderedDict([('classification__ccp_alpha', 0.004661715074460674), ('classification__class_weight', None), ('text__max_features', 96)])\n",
      "Test F1 score for fold 5: 0.147414416508645\n",
      "\n",
      "Validation F1 score for fold 6: 0.1436668892721179\n",
      "Best hyperparameters for fold 6: OrderedDict([('classification__ccp_alpha', 0.0029726280860927086), ('classification__class_weight', None), ('text__max_features', 81)])\n",
      "Test F1 score for fold 6: 0.12204145074576953\n",
      "\n",
      "Validation F1 score for fold 7: 0.16038297449939873\n",
      "Best hyperparameters for fold 7: OrderedDict([('classification__ccp_alpha', 0.0011841710806825415), ('classification__class_weight', 'balanced'), ('text__max_features', 65)])\n",
      "Test F1 score for fold 7: 0.14075389359398566\n",
      "\n",
      "Validation F1 score for fold 8: 0.11613553487350892\n",
      "Best hyperparameters for fold 8: OrderedDict([('classification__ccp_alpha', 0.004875295177385762), ('classification__class_weight', None), ('text__max_features', 77)])\n",
      "Test F1 score for fold 8: 0.11448425628409709\n",
      "\n",
      "Validation F1 score for fold 9: 0.12745207607977982\n",
      "Best hyperparameters for fold 9: OrderedDict([('classification__ccp_alpha', 0.010147840509961582), ('classification__class_weight', None), ('text__max_features', 98)])\n",
      "Test F1 score for fold 9: 0.13644238382349766\n",
      "\n",
      "Validation F1 score for fold 10: 0.104014233839452\n",
      "Best hyperparameters for fold 10: OrderedDict([('classification__ccp_alpha', 0.011821498555480408), ('classification__class_weight', None), ('text__max_features', 86)])\n",
      "Test F1 score for fold 10: 0.08461550750198094\n",
      "\n",
      "----------------------\n",
      "Average validation F1 score: 0.13212231792990417\n",
      "Average test F1 score: 0.1246122720899342\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=120)\n",
    "pipeline = Pipeline(steps=[])\n",
    "pipeline.steps.append(('text',TfidfVectorizer(ngram_range=(1,2))))\n",
    "pipeline.steps.append(('classification',clf))\n",
    "search_space = dict()\n",
    "search_space['text__max_features'] = Integer(1,100)\n",
    "search_space['classification__ccp_alpha'] = Real(0,0.02)\n",
    "search_space['classification__class_weight'] = Categorical(['balanced',None])\n",
    "nested_cv(corpus, pd.Series(train_data.target), pipeline, search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Doc2Vec\n",
    "\n",
    "Word2Vec and Doc2Vec are deep learning modules for NLP that uses shallow neural networks (with 1 hidden layer).\n",
    "\n",
    "Word2Vec involves deriving new features for every word under fixed-set of dimensions.\n",
    "\n",
    "<b>Neural-network methods of Word2Vec</b>:\n",
    "1. <b>Continuous Bag of Words (CBOW)</b>\n",
    "\n",
    "![image.png](https://1.bp.blogspot.com/-nZFc7P6o3Yc/XQo2cYPM_ZI/AAAAAAAABxM/XBqYSa06oyQ_sxQzPcgnUxb5msRwDrJrQCLcBGAs/s1600/image001.png)\n",
    "\n",
    "- Using odd number window size, center word is defined as target with left and right context as inputs in sliding window fashion.\n",
    "\n",
    "2. <b>SkipGram</b>\n",
    "\n",
    "![image.png](https://1.bp.blogspot.com/-Vz5pLuZ49K8/XV0ErlMtdDI/AAAAAAAAB0A/FIM74z__LAUkCqpW12ViAnGX8Br56W2PQCEwYBhgL/s1600/image001.png)\n",
    "\n",
    "- Using odd number window size, left and right context is defined as target with center word as inputs in sliding window fashion.\n",
    "\n",
    "![image.png](https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS%3A613966665486361%401523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png)\n",
    "\n",
    "From the diagram above, both methods are similar but with a difference in neural network architecture.\n",
    "\n",
    "Doc2Vec is another word embedding technique that is mostly used for sentences, which is more commonly applied in practice. \n",
    "\n",
    "Doc2Vec reduces dimensions created for every word from using Word2Vec by either \"row\" averaging or summation to represent single feature.\n",
    "\n",
    "Unlike Word2Vec, Doc2Vec is much faster algorithm, since no memory is required to store word vectors.\n",
    "\n",
    "<b>Algorithms of Doc2Vec</b>:\n",
    "\n",
    "1. Distributed Memory\n",
    "\n",
    "![image.png](https://miro.medium.com/max/640/0%2Ax-gtU4UlO8FAsRvL.)\n",
    "\n",
    "2. Distributed Bag of Words\n",
    "\n",
    "![image.png](https://miro.medium.com/max/640/0%2ANtIsrbd4VQzUKVKr.)\n",
    "\n",
    "Comparing both algorithms above, distributed memory model remembers what is missing from the current context — or as the topic of the paragraph, while distributed bag of words model is similar to SkipGram \n",
    "\n",
    "<b>Advantages</b>:\n",
    "\n",
    "1. Semantic information (meaning of similar words) is retained\n",
    "\n",
    "2. Reduces sparsity\n",
    "\n",
    "<b>Disadvantages</b>:\n",
    "\n",
    "1. Derived features are difficult to interpret\n",
    "\n",
    "<b>Important hyperparameters for Doc2Vec</b>:\n",
    "1. window: Size of hidden layer in neural network\n",
    "2. vector_size: Size of features\n",
    "3. dbow_words: 1 for training word vectors in skip gram together with DBOW for doc-vector training vs 0 for training doc-vector directly (usually faster).\n",
    "4. dm_mean: Method to use for handling context of word vectors (Average (1) or summation (0))\n",
    "5. dm: Type of algorithm to use for training paragraph vectors (Distributed memory (1) vs Distributed bag of words (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [06:48<00:00, 27.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "tqdm.pandas()\n",
    "def text_lemmatize(text):\n",
    "    review = re.sub('[^a-zA-Z]',' ', text).lower().split()\n",
    "    review = [wnl.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    return review\n",
    "corpus = pd.Series(train_data.data).progress_map(lambda x: text_lemmatize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(X, y, pipeline, search_space = None):\n",
    "    num_folds = 10\n",
    "    skfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=120)\n",
    "    val_f1, test_f1 = [], []\n",
    "    for fold, (outer_train_idx, outer_test_idx) in enumerate(skfold.split(X, y)):\n",
    "        X_train = X.iloc[outer_train_idx].reset_index(drop=True)\n",
    "        y_train = y.iloc[outer_train_idx].reset_index(drop=True)\n",
    "        X_test = X.iloc[outer_test_idx].reset_index(drop=True)\n",
    "        y_test = y.iloc[outer_test_idx].reset_index(drop=True)\n",
    "        search = BayesSearchCV(estimator=pipeline, search_spaces=search_space, cv=3, n_iter=10,scoring= make_scorer(f1_score, average='macro'),refit=True, n_jobs=3)\n",
    "        search.fit(X_train,y_train)\n",
    "        val_f1.append(search.best_score_)\n",
    "        print(f'Validation F1 score for fold {fold+1}:',search.best_score_)\n",
    "        print(f'Best hyperparameters for fold {fold+1}:', search.best_params_)\n",
    "        y_pred = search.best_estimator_.predict(X_test)\n",
    "        test_f1.append(f1_score(y_test,y_pred, average='macro'))\n",
    "        print(f'Test F1 score for fold {fold+1}:',f1_score(y_test,y_pred, average='macro'))\n",
    "        print()\n",
    "    print('----------------------')\n",
    "    print('Average validation F1 score:', np.mean(val_f1))\n",
    "    print('Average test F1 score:', np.mean(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, window_size=5, vector_size=100, sg=0):\n",
    "        self.window_size=window_size\n",
    "        self.vector_size=vector_size\n",
    "        self.sg = sg\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X)]\n",
    "        self.model = Doc2Vec(documents, window=self.window_size, vector_size=self.vector_size, dbow_words=self.sg, dm_mean=1)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        corpus_list=[]\n",
    "        for i in range(len(X)):\n",
    "            corpus_list.append(self.model.infer_vector(X.iloc[i]))\n",
    "        data = pd.DataFrame(corpus_list)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:39<00:00, 288.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181867</td>\n",
       "      <td>0.142621</td>\n",
       "      <td>0.160889</td>\n",
       "      <td>-0.059451</td>\n",
       "      <td>-0.185729</td>\n",
       "      <td>-0.384308</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>0.058495</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.276120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309787</td>\n",
       "      <td>-0.019852</td>\n",
       "      <td>-0.595044</td>\n",
       "      <td>-0.495648</td>\n",
       "      <td>-0.194946</td>\n",
       "      <td>0.115274</td>\n",
       "      <td>-0.745542</td>\n",
       "      <td>0.114599</td>\n",
       "      <td>-0.136785</td>\n",
       "      <td>-0.262709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.254696</td>\n",
       "      <td>-0.202066</td>\n",
       "      <td>0.326909</td>\n",
       "      <td>0.183216</td>\n",
       "      <td>0.108386</td>\n",
       "      <td>-0.020028</td>\n",
       "      <td>-0.388124</td>\n",
       "      <td>-0.143820</td>\n",
       "      <td>0.174458</td>\n",
       "      <td>0.170976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366750</td>\n",
       "      <td>0.252011</td>\n",
       "      <td>-0.630868</td>\n",
       "      <td>-0.320176</td>\n",
       "      <td>-0.153325</td>\n",
       "      <td>-0.037158</td>\n",
       "      <td>-0.275498</td>\n",
       "      <td>0.586989</td>\n",
       "      <td>-0.539745</td>\n",
       "      <td>0.174718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035048</td>\n",
       "      <td>-0.039179</td>\n",
       "      <td>0.297219</td>\n",
       "      <td>0.481677</td>\n",
       "      <td>-0.312172</td>\n",
       "      <td>-0.284266</td>\n",
       "      <td>0.595817</td>\n",
       "      <td>0.312010</td>\n",
       "      <td>-0.078611</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455461</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>-1.011400</td>\n",
       "      <td>-0.095309</td>\n",
       "      <td>-0.093512</td>\n",
       "      <td>-0.437727</td>\n",
       "      <td>-0.355270</td>\n",
       "      <td>0.302072</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.318478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.032163</td>\n",
       "      <td>-0.101395</td>\n",
       "      <td>-0.337188</td>\n",
       "      <td>-0.140559</td>\n",
       "      <td>0.078657</td>\n",
       "      <td>0.274490</td>\n",
       "      <td>0.251014</td>\n",
       "      <td>0.254533</td>\n",
       "      <td>-0.079445</td>\n",
       "      <td>-0.406573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140821</td>\n",
       "      <td>0.188404</td>\n",
       "      <td>-0.294763</td>\n",
       "      <td>0.082081</td>\n",
       "      <td>-0.094968</td>\n",
       "      <td>-0.223158</td>\n",
       "      <td>-0.258877</td>\n",
       "      <td>0.289686</td>\n",
       "      <td>0.038878</td>\n",
       "      <td>-0.347468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.225668</td>\n",
       "      <td>0.227880</td>\n",
       "      <td>0.239966</td>\n",
       "      <td>0.681074</td>\n",
       "      <td>-0.313954</td>\n",
       "      <td>-0.018924</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.287880</td>\n",
       "      <td>0.020320</td>\n",
       "      <td>-0.435511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552227</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>-0.478643</td>\n",
       "      <td>-0.274663</td>\n",
       "      <td>-0.139647</td>\n",
       "      <td>-0.374686</td>\n",
       "      <td>-0.334586</td>\n",
       "      <td>0.066808</td>\n",
       "      <td>-0.552995</td>\n",
       "      <td>0.044064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>0.196941</td>\n",
       "      <td>0.613343</td>\n",
       "      <td>0.549250</td>\n",
       "      <td>1.225908</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>-0.303703</td>\n",
       "      <td>0.371028</td>\n",
       "      <td>-0.109360</td>\n",
       "      <td>1.115787</td>\n",
       "      <td>-0.310815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.625617</td>\n",
       "      <td>-0.119782</td>\n",
       "      <td>-0.515483</td>\n",
       "      <td>-0.186371</td>\n",
       "      <td>-0.818890</td>\n",
       "      <td>0.951120</td>\n",
       "      <td>-0.990050</td>\n",
       "      <td>0.567417</td>\n",
       "      <td>0.040211</td>\n",
       "      <td>1.155560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>-0.197742</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.167696</td>\n",
       "      <td>0.160393</td>\n",
       "      <td>-0.025061</td>\n",
       "      <td>-0.496154</td>\n",
       "      <td>-0.179802</td>\n",
       "      <td>-0.189870</td>\n",
       "      <td>0.431367</td>\n",
       "      <td>-0.382585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621502</td>\n",
       "      <td>-0.362972</td>\n",
       "      <td>-0.635916</td>\n",
       "      <td>0.044675</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>-0.371604</td>\n",
       "      <td>-0.340396</td>\n",
       "      <td>0.327096</td>\n",
       "      <td>-0.128715</td>\n",
       "      <td>0.196688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>-0.255346</td>\n",
       "      <td>-0.360278</td>\n",
       "      <td>0.104715</td>\n",
       "      <td>0.717931</td>\n",
       "      <td>-0.148578</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>0.190778</td>\n",
       "      <td>-0.350770</td>\n",
       "      <td>0.071576</td>\n",
       "      <td>0.191106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025464</td>\n",
       "      <td>0.133005</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>-0.447190</td>\n",
       "      <td>-0.369890</td>\n",
       "      <td>-0.139451</td>\n",
       "      <td>-0.518170</td>\n",
       "      <td>0.230131</td>\n",
       "      <td>-0.522328</td>\n",
       "      <td>0.261976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>0.662580</td>\n",
       "      <td>0.442421</td>\n",
       "      <td>0.729126</td>\n",
       "      <td>-0.009726</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.483847</td>\n",
       "      <td>-0.341213</td>\n",
       "      <td>0.329171</td>\n",
       "      <td>-0.173751</td>\n",
       "      <td>-0.705158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.723478</td>\n",
       "      <td>0.335752</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>0.321691</td>\n",
       "      <td>0.176673</td>\n",
       "      <td>-0.209002</td>\n",
       "      <td>-0.387193</td>\n",
       "      <td>0.485543</td>\n",
       "      <td>-0.016760</td>\n",
       "      <td>0.189186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>-0.102361</td>\n",
       "      <td>0.105837</td>\n",
       "      <td>0.075483</td>\n",
       "      <td>-0.033384</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.047762</td>\n",
       "      <td>-0.203433</td>\n",
       "      <td>0.027190</td>\n",
       "      <td>0.326384</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040235</td>\n",
       "      <td>-0.059368</td>\n",
       "      <td>-0.303611</td>\n",
       "      <td>-0.184565</td>\n",
       "      <td>-0.234314</td>\n",
       "      <td>0.314922</td>\n",
       "      <td>-0.266618</td>\n",
       "      <td>-0.084139</td>\n",
       "      <td>-0.166279</td>\n",
       "      <td>-0.025660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.181867  0.142621  0.160889 -0.059451 -0.185729 -0.384308  0.070571   \n",
       "1     -0.254696 -0.202066  0.326909  0.183216  0.108386 -0.020028 -0.388124   \n",
       "2     -0.035048 -0.039179  0.297219  0.481677 -0.312172 -0.284266  0.595817   \n",
       "3     -0.032163 -0.101395 -0.337188 -0.140559  0.078657  0.274490  0.251014   \n",
       "4      0.225668  0.227880  0.239966  0.681074 -0.313954 -0.018924  0.005046   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11309  0.196941  0.613343  0.549250  1.225908  0.017679 -0.303703  0.371028   \n",
       "11310 -0.197742  0.203054  0.167696  0.160393 -0.025061 -0.496154 -0.179802   \n",
       "11311 -0.255346 -0.360278  0.104715  0.717931 -0.148578  0.033314  0.190778   \n",
       "11312  0.662580  0.442421  0.729126 -0.009726 -0.626188 -0.483847 -0.341213   \n",
       "11313 -0.102361  0.105837  0.075483 -0.033384 -0.014208  0.047762 -0.203433   \n",
       "\n",
       "             7         8         9   ...        40        41        42  \\\n",
       "0      0.058495  0.000561 -0.276120  ... -0.309787 -0.019852 -0.595044   \n",
       "1     -0.143820  0.174458  0.170976  ... -0.366750  0.252011 -0.630868   \n",
       "2      0.312010 -0.078611 -0.001274  ... -0.455461  0.064141 -1.011400   \n",
       "3      0.254533 -0.079445 -0.406573  ... -0.140821  0.188404 -0.294763   \n",
       "4      0.287880  0.020320 -0.435511  ... -0.552227  0.029562 -0.478643   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "11309 -0.109360  1.115787 -0.310815  ... -0.625617 -0.119782 -0.515483   \n",
       "11310 -0.189870  0.431367 -0.382585  ... -0.621502 -0.362972 -0.635916   \n",
       "11311 -0.350770  0.071576  0.191106  ... -0.025464  0.133005 -0.040903   \n",
       "11312  0.329171 -0.173751 -0.705158  ... -0.723478  0.335752  0.030538   \n",
       "11313  0.027190  0.326384  0.056587  ... -0.040235 -0.059368 -0.303611   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "0     -0.495648 -0.194946  0.115274 -0.745542  0.114599 -0.136785 -0.262709  \n",
       "1     -0.320176 -0.153325 -0.037158 -0.275498  0.586989 -0.539745  0.174718  \n",
       "2     -0.095309 -0.093512 -0.437727 -0.355270  0.302072  0.369565  0.318478  \n",
       "3      0.082081 -0.094968 -0.223158 -0.258877  0.289686  0.038878 -0.347468  \n",
       "4     -0.274663 -0.139647 -0.374686 -0.334586  0.066808 -0.552995  0.044064  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "11309 -0.186371 -0.818890  0.951120 -0.990050  0.567417  0.040211  1.155560  \n",
       "11310  0.044675  0.204900 -0.371604 -0.340396  0.327096 -0.128715  0.196688  \n",
       "11311 -0.447190 -0.369890 -0.139451 -0.518170  0.230131 -0.522328  0.261976  \n",
       "11312  0.321691  0.176673 -0.209002 -0.387193  0.485543 -0.016760  0.189186  \n",
       "11313 -0.184565 -0.234314  0.314922 -0.266618 -0.084139 -0.166279 -0.025660  \n",
       "\n",
       "[11314 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Continuous Bag of Words\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(documents, window=5, vector_size=50, dbow_words=0, dm_mean=1)\n",
    "vec_list = []\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    vec_list.append(model.infer_vector(corpus.iloc[i]))\n",
    "data = pd.DataFrame(vec_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score for fold 1: 0.31464911188966166\n",
      "Best hyperparameters for fold 1: OrderedDict([('classification__ccp_alpha', 0.0014507615937854993), ('classification__class_weight', 'balanced'), ('text__vector_size', 28), ('text__window_size', 5)])\n",
      "Test F1 score for fold 1: 0.30708073075526265\n",
      "\n",
      "Validation F1 score for fold 2: 0.2802194238270943\n",
      "Best hyperparameters for fold 2: OrderedDict([('classification__ccp_alpha', 0.002214737395726681), ('classification__class_weight', None), ('text__vector_size', 10), ('text__window_size', 5)])\n",
      "Test F1 score for fold 2: 0.2584749714270034\n",
      "\n",
      "Validation F1 score for fold 3: 0.28429350539403836\n",
      "Best hyperparameters for fold 3: OrderedDict([('classification__ccp_alpha', 0.0013276722768080765), ('classification__class_weight', None), ('text__vector_size', 41), ('text__window_size', 7)])\n",
      "Test F1 score for fold 3: 0.25178622379934185\n",
      "\n",
      "Validation F1 score for fold 4: 0.25682745921600997\n",
      "Best hyperparameters for fold 4: OrderedDict([('classification__ccp_alpha', 0.002565999762168226), ('classification__class_weight', None), ('text__vector_size', 15), ('text__window_size', 9)])\n",
      "Test F1 score for fold 4: 0.261062964123781\n",
      "\n",
      "Validation F1 score for fold 5: 0.25806171140135076\n",
      "Best hyperparameters for fold 5: OrderedDict([('classification__ccp_alpha', 0.0024945962348735078), ('classification__class_weight', 'balanced'), ('text__vector_size', 66), ('text__window_size', 3)])\n",
      "Test F1 score for fold 5: 0.23572739490752967\n",
      "\n",
      "Validation F1 score for fold 6: 0.33989915721939196\n",
      "Best hyperparameters for fold 6: OrderedDict([('classification__ccp_alpha', 0.00012590108464514808), ('classification__class_weight', 'balanced'), ('text__vector_size', 31), ('text__window_size', 5)])\n",
      "Test F1 score for fold 6: 0.35923263752174994\n",
      "\n",
      "Validation F1 score for fold 7: 0.32295718659612616\n",
      "Best hyperparameters for fold 7: OrderedDict([('classification__ccp_alpha', 0.0014587099958858788), ('classification__class_weight', 'balanced'), ('text__vector_size', 86), ('text__window_size', 3)])\n",
      "Test F1 score for fold 7: 0.3134092330923162\n",
      "\n",
      "Validation F1 score for fold 8: 0.29281311031151924\n",
      "Best hyperparameters for fold 8: OrderedDict([('classification__ccp_alpha', 0.0018242514378714895), ('classification__class_weight', 'balanced'), ('text__vector_size', 86), ('text__window_size', 3)])\n",
      "Test F1 score for fold 8: 0.29101708678045846\n",
      "\n",
      "Validation F1 score for fold 9: 0.3236236741113539\n",
      "Best hyperparameters for fold 9: OrderedDict([('classification__ccp_alpha', 0.00025435979826351715), ('classification__class_weight', None), ('text__vector_size', 62), ('text__window_size', 5)])\n",
      "Test F1 score for fold 9: 0.34783849147624635\n",
      "\n",
      "Validation F1 score for fold 10: 0.2613882146831265\n",
      "Best hyperparameters for fold 10: OrderedDict([('classification__ccp_alpha', 0.0026935972015508843), ('classification__class_weight', None), ('text__vector_size', 28), ('text__window_size', 5)])\n",
      "Test F1 score for fold 10: 0.2727317511786102\n",
      "\n",
      "----------------------\n",
      "Average validation F1 score: 0.2934732554649673\n",
      "Average test F1 score: 0.28983614850622996\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=120)\n",
    "pipeline = Pipeline(steps=[])\n",
    "pipeline.steps.append(('text',Word2VecTransformer()))\n",
    "pipeline.steps.append(('classification',clf))\n",
    "search_space = dict()\n",
    "search_space['text__window_size'] = Categorical([3,5,7,9])\n",
    "search_space['text__vector_size'] = Integer(1,100)\n",
    "search_space['classification__ccp_alpha'] = Real(0,0.02)\n",
    "search_space['classification__class_weight'] = Categorical(['balanced',None])\n",
    "nested_cv(corpus, pd.Series(train_data.target), pipeline, search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:38<00:00, 290.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138296</td>\n",
       "      <td>0.310877</td>\n",
       "      <td>-0.003023</td>\n",
       "      <td>-0.098006</td>\n",
       "      <td>-0.154416</td>\n",
       "      <td>-0.077633</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>0.416268</td>\n",
       "      <td>-0.016233</td>\n",
       "      <td>-0.279939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430208</td>\n",
       "      <td>0.129288</td>\n",
       "      <td>-0.318273</td>\n",
       "      <td>-0.403427</td>\n",
       "      <td>-0.080636</td>\n",
       "      <td>0.029839</td>\n",
       "      <td>-0.804690</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>-0.094002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.322533</td>\n",
       "      <td>-0.228493</td>\n",
       "      <td>0.259658</td>\n",
       "      <td>0.583815</td>\n",
       "      <td>0.054585</td>\n",
       "      <td>-0.410566</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>-0.061435</td>\n",
       "      <td>0.061736</td>\n",
       "      <td>-0.183913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710538</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>-0.127719</td>\n",
       "      <td>-0.374665</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>-0.308150</td>\n",
       "      <td>0.499244</td>\n",
       "      <td>-0.461537</td>\n",
       "      <td>0.258391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017253</td>\n",
       "      <td>-0.300993</td>\n",
       "      <td>0.078815</td>\n",
       "      <td>0.587306</td>\n",
       "      <td>-0.253314</td>\n",
       "      <td>-0.877639</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.601593</td>\n",
       "      <td>-0.214956</td>\n",
       "      <td>-0.280966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481166</td>\n",
       "      <td>-0.050697</td>\n",
       "      <td>-0.648011</td>\n",
       "      <td>-0.406621</td>\n",
       "      <td>0.348724</td>\n",
       "      <td>-0.056114</td>\n",
       "      <td>-0.155708</td>\n",
       "      <td>-0.284408</td>\n",
       "      <td>0.228785</td>\n",
       "      <td>0.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.112147</td>\n",
       "      <td>-0.147457</td>\n",
       "      <td>-0.052790</td>\n",
       "      <td>0.129133</td>\n",
       "      <td>-0.019868</td>\n",
       "      <td>-0.048773</td>\n",
       "      <td>0.186313</td>\n",
       "      <td>0.291806</td>\n",
       "      <td>-0.417658</td>\n",
       "      <td>-0.315570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152403</td>\n",
       "      <td>0.243233</td>\n",
       "      <td>-0.547521</td>\n",
       "      <td>-0.151617</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>-0.062400</td>\n",
       "      <td>-0.196100</td>\n",
       "      <td>0.390207</td>\n",
       "      <td>-0.152791</td>\n",
       "      <td>-0.245113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086882</td>\n",
       "      <td>0.430148</td>\n",
       "      <td>-0.073857</td>\n",
       "      <td>0.634513</td>\n",
       "      <td>-0.360836</td>\n",
       "      <td>-0.342152</td>\n",
       "      <td>0.314116</td>\n",
       "      <td>0.309371</td>\n",
       "      <td>0.206878</td>\n",
       "      <td>-0.316622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.659270</td>\n",
       "      <td>-0.253001</td>\n",
       "      <td>-0.315521</td>\n",
       "      <td>-0.161839</td>\n",
       "      <td>-0.295772</td>\n",
       "      <td>-0.184502</td>\n",
       "      <td>-0.257272</td>\n",
       "      <td>0.101284</td>\n",
       "      <td>-0.545335</td>\n",
       "      <td>0.042074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>-0.203259</td>\n",
       "      <td>0.332130</td>\n",
       "      <td>0.381063</td>\n",
       "      <td>1.195262</td>\n",
       "      <td>-0.286618</td>\n",
       "      <td>-0.158270</td>\n",
       "      <td>0.479994</td>\n",
       "      <td>-0.150410</td>\n",
       "      <td>1.007477</td>\n",
       "      <td>-0.013528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.733101</td>\n",
       "      <td>-0.054535</td>\n",
       "      <td>-0.182382</td>\n",
       "      <td>0.214754</td>\n",
       "      <td>-0.824925</td>\n",
       "      <td>0.684483</td>\n",
       "      <td>-1.446060</td>\n",
       "      <td>0.552993</td>\n",
       "      <td>0.200208</td>\n",
       "      <td>0.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>-0.334115</td>\n",
       "      <td>0.119862</td>\n",
       "      <td>-0.094698</td>\n",
       "      <td>0.166379</td>\n",
       "      <td>0.210553</td>\n",
       "      <td>-0.534351</td>\n",
       "      <td>0.117241</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>0.214325</td>\n",
       "      <td>-0.523951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487798</td>\n",
       "      <td>-0.247501</td>\n",
       "      <td>-0.445998</td>\n",
       "      <td>-0.071153</td>\n",
       "      <td>-0.260226</td>\n",
       "      <td>-0.400416</td>\n",
       "      <td>-0.331814</td>\n",
       "      <td>0.413073</td>\n",
       "      <td>0.237487</td>\n",
       "      <td>0.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>-0.095471</td>\n",
       "      <td>-0.068837</td>\n",
       "      <td>0.139592</td>\n",
       "      <td>0.861653</td>\n",
       "      <td>-0.168663</td>\n",
       "      <td>-0.147079</td>\n",
       "      <td>0.390835</td>\n",
       "      <td>-0.215389</td>\n",
       "      <td>0.098699</td>\n",
       "      <td>0.097101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042169</td>\n",
       "      <td>0.252173</td>\n",
       "      <td>0.097161</td>\n",
       "      <td>-0.351590</td>\n",
       "      <td>-0.181138</td>\n",
       "      <td>-0.352563</td>\n",
       "      <td>-0.658175</td>\n",
       "      <td>0.259115</td>\n",
       "      <td>-0.317019</td>\n",
       "      <td>0.102958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>0.342822</td>\n",
       "      <td>0.470287</td>\n",
       "      <td>0.440438</td>\n",
       "      <td>-0.141964</td>\n",
       "      <td>-0.583460</td>\n",
       "      <td>-0.216313</td>\n",
       "      <td>-0.118912</td>\n",
       "      <td>0.433664</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>-0.473858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.448919</td>\n",
       "      <td>0.274798</td>\n",
       "      <td>0.052984</td>\n",
       "      <td>0.042835</td>\n",
       "      <td>-0.158311</td>\n",
       "      <td>-0.246133</td>\n",
       "      <td>-0.033715</td>\n",
       "      <td>0.465782</td>\n",
       "      <td>0.139712</td>\n",
       "      <td>0.428265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>-0.071545</td>\n",
       "      <td>0.221740</td>\n",
       "      <td>0.040326</td>\n",
       "      <td>0.211525</td>\n",
       "      <td>-0.026495</td>\n",
       "      <td>0.047301</td>\n",
       "      <td>-0.153529</td>\n",
       "      <td>0.077639</td>\n",
       "      <td>0.341068</td>\n",
       "      <td>-0.113328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368810</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>-0.157700</td>\n",
       "      <td>-0.078804</td>\n",
       "      <td>-0.271824</td>\n",
       "      <td>0.186355</td>\n",
       "      <td>-0.337740</td>\n",
       "      <td>-0.054916</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>-0.101677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.138296  0.310877 -0.003023 -0.098006 -0.154416 -0.077633  0.149321   \n",
       "1     -0.322533 -0.228493  0.259658  0.583815  0.054585 -0.410566  0.161045   \n",
       "2      0.017253 -0.300993  0.078815  0.587306 -0.253314 -0.877639  0.610329   \n",
       "3     -0.112147 -0.147457 -0.052790  0.129133 -0.019868 -0.048773  0.186313   \n",
       "4      0.086882  0.430148 -0.073857  0.634513 -0.360836 -0.342152  0.314116   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11309 -0.203259  0.332130  0.381063  1.195262 -0.286618 -0.158270  0.479994   \n",
       "11310 -0.334115  0.119862 -0.094698  0.166379  0.210553 -0.534351  0.117241   \n",
       "11311 -0.095471 -0.068837  0.139592  0.861653 -0.168663 -0.147079  0.390835   \n",
       "11312  0.342822  0.470287  0.440438 -0.141964 -0.583460 -0.216313 -0.118912   \n",
       "11313 -0.071545  0.221740  0.040326  0.211525 -0.026495  0.047301 -0.153529   \n",
       "\n",
       "             7         8         9   ...        40        41        42  \\\n",
       "0      0.416268 -0.016233 -0.279939  ... -0.430208  0.129288 -0.318273   \n",
       "1     -0.061435  0.061736 -0.183913  ... -0.710538  0.273902 -0.127719   \n",
       "2      0.601593 -0.214956 -0.280966  ... -0.481166 -0.050697 -0.648011   \n",
       "3      0.291806 -0.417658 -0.315570  ... -0.152403  0.243233 -0.547521   \n",
       "4      0.309371  0.206878 -0.316622  ... -0.659270 -0.253001 -0.315521   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "11309 -0.150410  1.007477 -0.013528  ... -0.733101 -0.054535 -0.182382   \n",
       "11310  0.087344  0.214325 -0.523951  ... -0.487798 -0.247501 -0.445998   \n",
       "11311 -0.215389  0.098699  0.097101  ... -0.042169  0.252173  0.097161   \n",
       "11312  0.433664  0.012252 -0.473858  ... -0.448919  0.274798  0.052984   \n",
       "11313  0.077639  0.341068 -0.113328  ... -0.368810  0.006762 -0.157700   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "0     -0.403427 -0.080636  0.029839 -0.804690  0.010699  0.195876 -0.094002  \n",
       "1     -0.374665  0.014058 -0.084448 -0.308150  0.499244 -0.461537  0.258391  \n",
       "2     -0.406621  0.348724 -0.056114 -0.155708 -0.284408  0.228785  0.338700  \n",
       "3     -0.151617  0.022199 -0.062400 -0.196100  0.390207 -0.152791 -0.245113  \n",
       "4     -0.161839 -0.295772 -0.184502 -0.257272  0.101284 -0.545335  0.042074  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "11309  0.214754 -0.824925  0.684483 -1.446060  0.552993  0.200208  0.639000  \n",
       "11310 -0.071153 -0.260226 -0.400416 -0.331814  0.413073  0.237487  0.409700  \n",
       "11311 -0.351590 -0.181138 -0.352563 -0.658175  0.259115 -0.317019  0.102958  \n",
       "11312  0.042835 -0.158311 -0.246133 -0.033715  0.465782  0.139712  0.428265  \n",
       "11313 -0.078804 -0.271824  0.186355 -0.337740 -0.054916  0.032795 -0.101677  \n",
       "\n",
       "[11314 rows x 50 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SkipGram\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(documents, window=5, vector_size=50, dbow_words=1, dm_mean=1)\n",
    "vec_list = []\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    vec_list.append(model.infer_vector(corpus.iloc[i]))\n",
    "data = pd.DataFrame(vec_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score for fold 1: 0.14679261130596402\n",
      "Best hyperparameters for fold 1: OrderedDict([('classification__ccp_alpha', 0.004414685066848233), ('classification__class_weight', None), ('text__vector_size', 34), ('text__window_size', 9)])\n",
      "Test F1 score for fold 1: 0.1367525825302221\n",
      "\n",
      "Validation F1 score for fold 2: 0.26865655194851895\n",
      "Best hyperparameters for fold 2: OrderedDict([('classification__ccp_alpha', 0.0011607221554817196), ('classification__class_weight', None), ('text__vector_size', 83), ('text__window_size', 9)])\n",
      "Test F1 score for fold 2: 0.23049590128547165\n",
      "\n",
      "Validation F1 score for fold 3: 0.32601892430323814\n",
      "Best hyperparameters for fold 3: OrderedDict([('classification__ccp_alpha', 0.00037908224336926873), ('classification__class_weight', None), ('text__vector_size', 80), ('text__window_size', 5)])\n",
      "Test F1 score for fold 3: 0.3359490583581755\n",
      "\n",
      "Validation F1 score for fold 4: 0.29275517102371446\n",
      "Best hyperparameters for fold 4: OrderedDict([('classification__ccp_alpha', 0.0014593106319398834), ('classification__class_weight', None), ('text__vector_size', 47), ('text__window_size', 3)])\n",
      "Test F1 score for fold 4: 0.3153951781738517\n",
      "\n",
      "Validation F1 score for fold 5: 0.31470103591435766\n",
      "Best hyperparameters for fold 5: OrderedDict([('classification__ccp_alpha', 0.0003460174961510432), ('classification__class_weight', 'balanced'), ('text__vector_size', 81), ('text__window_size', 7)])\n",
      "Test F1 score for fold 5: 0.2785456115629981\n",
      "\n",
      "Validation F1 score for fold 6: 0.22373364675371202\n",
      "Best hyperparameters for fold 6: OrderedDict([('classification__ccp_alpha', 0.0039932863474799445), ('classification__class_weight', None), ('text__vector_size', 66), ('text__window_size', 3)])\n",
      "Test F1 score for fold 6: 0.20698807953382353\n",
      "\n",
      "Validation F1 score for fold 7: 0.3234950465801479\n",
      "Best hyperparameters for fold 7: OrderedDict([('classification__ccp_alpha', 0.00019946958712845135), ('classification__class_weight', None), ('text__vector_size', 34), ('text__window_size', 9)])\n",
      "Test F1 score for fold 7: 0.3225110962386407\n",
      "\n",
      "Validation F1 score for fold 8: 0.29256167225085106\n",
      "Best hyperparameters for fold 8: OrderedDict([('classification__ccp_alpha', 0.0009460595602115321), ('classification__class_weight', None), ('text__vector_size', 66), ('text__window_size', 7)])\n",
      "Test F1 score for fold 8: 0.2943358534791633\n",
      "\n",
      "Validation F1 score for fold 9: 0.3099432720096597\n",
      "Best hyperparameters for fold 9: OrderedDict([('classification__ccp_alpha', 0.0001255567012228576), ('classification__class_weight', None), ('text__vector_size', 94), ('text__window_size', 5)])\n",
      "Test F1 score for fold 9: 0.2948544613581904\n",
      "\n",
      "Validation F1 score for fold 10: 0.21595515924657538\n",
      "Best hyperparameters for fold 10: OrderedDict([('classification__ccp_alpha', 0.003861223010495013), ('classification__class_weight', 'balanced'), ('text__vector_size', 94), ('text__window_size', 3)])\n",
      "Test F1 score for fold 10: 0.2033656130899355\n",
      "\n",
      "----------------------\n",
      "Average validation F1 score: 0.2714613091336739\n",
      "Average test F1 score: 0.26191934356104724\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=120)\n",
    "pipeline = Pipeline(steps=[])\n",
    "pipeline.steps.append(('text',Word2VecTransformer(sg=1)))\n",
    "pipeline.steps.append(('classification',clf))\n",
    "search_space = dict()\n",
    "search_space['text__window_size'] = Categorical([3,5,7,9])\n",
    "search_space['text__vector_size'] = Integer(1,100)\n",
    "search_space['classification__ccp_alpha'] = Real(0,0.02)\n",
    "search_space['classification__class_weight'] = Categorical(['balanced',None])\n",
    "nested_cv(corpus, pd.Series(train_data.target), pipeline, search_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d9fb0276ebe726d3ef978e60152cc8db6cd06b5247c94959dcf1f3435a82596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
